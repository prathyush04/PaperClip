Representation Transferability in Neural Networks
Across Datasets and Tasks
Abstract
Deep neural networks, which are built from multiple layers with hierarchical
distributed representations, tend to learn low-level features in their initial layers
and shift to high-level features in subsequent layers. Transfer learning, multi-task
learning, and continual learning paradigms leverage this hierarchical distributed
representation to share knowledge across different datasets and tasks. This paper
studies the layer-wise transferability of representations in deep networks across
several datasets and tasks, noting interesting empirical observations.
1 Introduction
Deep networks, constructed with multiple layers and hierarchical distributed representations, learn
low-level features in initial layers and shift to high-level features as the network becomes deeper.
Generic hierarchical distributed representations allow for the sharing of knowledge across datasets
and tasks in paradigms such as transfer learning, multi-task learning, and continual learning. In
transfer learning, for example, the transfer of low-level features from one dataset to another can
boost performance on the target task when data is limited, provided that the datasets are related.
Transferring high-level features, with the learning of low-level features, can also be useful when the
tasks are similar but the data distributions differ slightly.
This paper studies the layer-wise transferability of representations in deep networks across several
datasets and tasks, and reports some interesting observations. First, we demonstrate that the layer-wise
transferability between datasets or tasks can be non-symmetric, with features learned from a source
dataset being more relevant to a target dataset, despite similar sizes. Secondly, the characteristics of
the datasets or tasks and their relationship have a greater effect on the layer-wise transferability of
representations than factors such as the network architecture. Third, we propose that the layer-wise
transferability of representations can be a proxy for measuring task relatedness. These observations
emphasize the importance of curriculum methods and structured approaches to designing systems
for multiple tasks that maximize knowledge transfer and minimize interference between datasets or
tasks.
2 Citation Networks
2.1 Methods
We have produced a citation graph using citation data from NeurIPS papers from SemanticScholar,
and institutional information about authors from AMiner. From the NeurIPS website, we first gathered
all paper titles from 2012 to 2021. We then mapped the paper titles to their Semantic Scholar paper
IDs using the Semantic Scholar Academic Graph (S2AG) API. Unmatched papers were manually
searched for, with all but one being found in the Semantic Scholar database. For each paper, we used
the S2AG API to identify authors, and the authors of their references.
We used AMiner to identify institutional information for each author. The 9460 NeurIPS papers
contain 135,941 authors, with institutions found for 83,515 (61%) of them. Papers lacking author
.
information were removed from our dataset. We then marked institutes automatically by country
name and common cities and regions in China. We supplemented automatic annotations with existing
regional matchings and added 364 additional rules for regional matching. We also removed major
multinational corporate labs. Of the remaining 5422 papers, we removed papers that were not from
China, the US, or Europe, or included collaborators from multiple regions, leaving us with 1792
papers. Finally, we calculated the average number and proportion of citations between papers from
each region.
2.2 Results
Our results show how American and Chinese papers fail to cite each other. While 60% of the data set
comes from American papers, they only compose 34% of Chinese citations. American citations of
Chinese papers are even more dramatic, with the 34% of the dataset coming from Chinese papers only
accounting for 9% of American citations. These numbers are even more significant when compared
to American citations of European papers; we found that American institutions cite European papers
more often than Chinese papers despite our dataset containing six times more Chinese papers than
European.
Each region tends to cite its own papers more often: China 21%, the USA 41%, and Europe 14%.
The separation between American and Chinese research is more pronounced than would be expected
based solely on regional preference. American and European research communities demonstrate
similar citation patterns with few citations to Chinese papers. Chinese institutions, on the other hand,
cite both American and European papers less than either of those regions.
USA China Europe
USA 41 9 12
China 34 21 6
Europe 15 9 14
Table 1: Proportion of papers from given regions citing other regions or endogenously. Values are in
percentage.
3 Limitations
The results presented here have some limitations. Firstly, while we have labeled the work of any
university located in the United States as American, it is possible that such labs still have close ties to
China, leading to an underestimate of the divide between US and Chinese AI research. Secondly, we
have excluded papers where author information was not available on AMiner, a Chinese company,
and therefore, there could be more Chinese papers in our dataset than we have determined. The 43%
of discarded papers due to missing author information also likely represent a biased sample.
4 Consequences
While American and Chinese researchers publish in the same venues, they represent two parallel
communities with limited impact on each otherâ€™s research. This can, partly, be attributed to differing
research interests arising from distinct cultural norms that influence research priorities. For instance,
multi-object tracking is an active area of research in China with large scale benchmarks, whereas,
concerns surrounding misuse of biometric data in North America have led researchers there to avoid
such research. Likewise, US researchers are heavily represented at conferences regarding fairness in
AI, while the Chinese are not.
This separation impacts not only the research topics, but also how they evolve. In addition, abstract
topics or architectures that are popular in one region may not be popular in the other. For example,
PCANet which is a popular image classification architecture has most of its 1200 citations from East
Asian institutions, while Deep Forests has most of its 600 citations from Chinese institutions.
Another limitation is related to differences in the approach to ethics. The North American and Euro-
pean AI communities have begun to publish research on the ethics of AI and have included systems
2
for reviewers to flag ethical concerns and ask authors to provide ethics statements. Engagement
with Chinese researchers in this topic remains limited, even though ethics statements from Chinese
AI institutions show many similarities to western ones. A clear example of this disconnect is the
Provisional Draft of the NeurIPS Code of Ethics where, at the time of initial publication, all the
authors were based in the US or Australia, but none were based in Asia. Although similar statements
exist across regions, disagreements in research practice still arise. One such example is where Duke
University stopped using the Duke-MTMC dataset because researchers had not obtained consent
from the students they collected images from, yet similar datasets like Market-1501 from China
continue to be used.
The divide between these two communities impacts individual researchers, the machine learning
community as a whole, and potentially the societies impacted by AI research, highlighting the need
for a discussion to overcome this barrier.
3