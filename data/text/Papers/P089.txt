Precise Requirements for the Validity of the Neural Tangent Kernel
Approximation
Abstract
This research investigates the conditions under which the neural tangent kernel (NTK) approximation remains
valid when employing the square loss function for model training. Within the framework of lazy training, as
introduced by Chizat et al., we demonstrate that a model, rescaled by a factor of α=O(T), maintains the validity
of the NTK approximation up to a training time of T. This finding refines the earlier result from Chizat et al.,
which necessitated a larger rescaling factor of α=O(T2), and establishes the preciseness of our established
bound.
1 Introduction
In contemporary machine learning practice, the weights wof expansive neural network models fw:Rdin→Rdoutare trained
using gradient-based optimizers. However, a comprehensive theoretical understanding remains elusive due to the non-linear nature
of the training dynamics, which complicates analysis. To bridge this gap, an approximation to these dynamics, termed the NTK
approximation, was introduced, and its validity for infinitely wide networks trained via gradient descent was demonstrated. The NTK
approximation has proven highly influential, offering theoretical insights into various phenomena, including deep learning’s capacity
to memorize training data, the manifestation of spectral bias in neural networks, and the differential generalization capabilities of
diverse architectures. Nevertheless, empirical evidence suggests that the training dynamics of neural networks frequently deviate
from the NTK approximation’s predictions. Consequently, it becomes crucial to delineate the precise conditions under which the
NTK approximation remains applicable. This paper seeks to address the following inquiry:
Is it possible to establish precise conditions that guarantee the validity of the NTK approximation?
1.1 The Lazy Training Framework
The work demonstrated that the NTK approximation is applicable to the training of any differentiable model, provided the model’s
outputs are rescaled appropriately. This rescaling ensures that significant changes in the model’s outputs can occur even with minor
adjustments to the weights. The validity of the NTK approximation for models of infinite width stems from this observation, as the
model is inherently rescaled as its width approaches infinity.
Consider a smoothly parameterized model h:Rp→ F , where Fis a separable Hilbert space. Let α >0be a parameter governing
the model’s rescaling, which should be considered large. We train the rescaled model αhusing gradient flow to minimize a smooth
loss function R:F →R+. The weights w(t)∈Rpare initialized at w(0) = w0and evolve according to the gradient flow:
dw
dt=−1
α2∇wR(αh(w(t))). (1)
Define the linear approximation of the model around the initial weights w0as:
¯h(w) =h(w0) +Dh(w0)(w−w0), (2)
where Dhis the first derivative of hwith respect to w. Let ¯w(t)be weights initialized at ¯w(0) = w0that evolve according to the
gradient flow from training the rescaled linearized model α¯h:
d¯w
dt=−1
α2∇¯wR(α¯h( ¯w(t))). (3)
The NTK approximation asserts that:
αh(w(t))≈α¯h( ¯w(t)). (4)
In essence, this implies that the linearization of the model hremains valid throughout the training process. This greatly simplifies
the analysis of training dynamics, as the model ¯his linear in its parameters, allowing the evolution of ¯h( ¯w)to be understood through
a kernel gradient flow in function space.
The validity of the NTK approximation is contingent on the magnitude of the rescaling parameter α. Intuitively, a larger α
implies that the weights need not deviate significantly from their initialization to induce substantial changes in the model’s output,
thereby prolonging the validity of the linearization. This regime of training, where weights remain close to their initialization,
is referred to as "lazy training." The following bound was established, where R0=R(αh(w0)))is the loss at initialization, and
κ=Tα−1Lip(Dh)√R0is a quantity that will also feature in our main results:
**Proposition 1.1.** Let R(y) =1
2∥y−y∗∥2
2be the square loss, where y∗∈ F are the target labels. Assume that hisLip(h)-
Lipschitz and that Dhis Lip (Dh)-Lipschitz in a ball of radius ρaround w0. Then, for any time 0≤T≤αρ/(Lip(h)√R0),
∥αh(w(T))−α¯h( ¯w(T))∥ ≤TLip(h)2κR0. (5)
Asαapproaches infinity, κtends to 0, rendering the right-hand side of the inequality small and validating the NTK approximation.
1.2 Our Contributions
Our primary contribution is the refinement of the bound for extended time scales. We establish the following theorem:
**Theorem 1.2 (NTK Approximation Error Bound).** Let R(y) =1
2∥y−y∗∥2
2be the square loss. Assume that DhisLip(Dh)-
Lipschitz in a ball of radius ρaround w0. Then, at any time 0≤T≤α2ρ2/R0,
∥αh(w(T))−α¯h( ¯w(T))∥ ≤min(6 κp
R0,8R0). (6)
Furthermore, we demonstrate that this bound is tight up to a constant factor.
**Theorem 1.3 (Converse to Theorem 1.2).** For any α,T,Lip(Dh), andR0, there exists a model h:R→R, a target y∗∈R, and
an initialization w0∈Rsuch that, for the risk R(y) =1
2(y−y∗)2, the initial risk is R(αh(w0)) =R0, the derivative map Dhis
Lip(Dh)-Lipschitz, and
∥αh(w(T))−α¯h( ¯w(T))∥ ≥min1
5κp
R0,1
5R0
. (7)
In contrast to prior work, our bound does not depend on the Lipschitz constant of h, and it exhibits a more favorable dependence on
T. Specifically, if Lip(Dh),Lip(h), and R0are bounded by constants, our result indicates that the NTK approximation, up to an
error of O(ϵ), holds for times T=O(αϵ), whereas the previously known bound was valid for T=O(√αϵ). Given the practical
interest in long training times T≫1, our result demonstrates that the NTK approximation is valid for significantly longer time
horizons than previously recognized.
2 Application to Neural Networks
The bound established in Theorem 1.2 is applicable to the lazy training of any differentiable model. As a specific example, we detail
its application to neural networks. We parameterize the networks in the mean-field regime, where the NTK approximation does not
hold even as the width approaches infinity. Consequently, the NTK approximation is valid only when training is conducted in the
lazy regime.
Letfw:Rd→Rbe a 2-layer network of width min the mean-field parametrization, with activation function σ:R→R,
fw(x) =1√mmX
i=1aiσ(√m⟨x, ui⟩). (8)
The weights are w= (a, U)fora= [a1, . . . , a m]andU= [u1, . . . , u m]. These are initialized at w0with i.i.d.
Unif[−1/√m,1/√m]entries. Given training data (x1, y1), . . . , (xn, yn), we train the weights of the network with the mean-
squared loss
L(w) =1
nnX
i=1ℓ(fw(xi), yi), ℓ(a, b) =1
2(a−b)2. (9)
In the Hilbert space notation, we let H=Rn, so that the gradient flow training dynamics with loss (6) correspond to the gradient
flow dynamics (1) with the following model and loss function
h(w) =1√n[fw(x1), . . . , f w(xn)]∈Rn, R (v) =1
2v−y√n2
2. (10)
Under certain regularity assumptions on the activation function (satisfied, for instance, by the sigmoid function) and a bound on the
weights, it can be shown that Lip (Dh)is bounded.
**Lemma 2.1 (Bound on Lip(Dh)for mean-field 2-layer network).** Suppose there exists a constant Ksuch that (i) the activation
function σis bounded and has bounded derivatives ∥σ∥∞,∥σ′∥∞,∥σ′′∥∞,∥σ′′′∥∞≤K, (ii) the weights have bounded norm
∥U∥a≤K, and (iii) the data points have bounded norm ∥x∥ ≤K. Then there exists a constant K′depending only on Ksuch that
Lip(Dh)≤K′. (11)
2
Since the assumptions of Theorem 1.2 are met, we obtain the following corollary for the lazy training dynamics of the 2-layer
mean-field network.
**Corollary 2.2 (Lazy training of 2-layer mean-field network).** Suppose the conditions of Lemma 2.1 hold, and also that the labels
are bounded in norm ∥y∥ ≤c. Then there exist constants C, c > 0depending only on Ksuch that for any time 0≤T≤cα2,
∥αh(w(T))−α¯h( ¯w(T))∥ ≤Cmin(T/α, 1). (12)
Training in the NTK parametrization corresponds to training the model√mfw, where fwis the network in the mean-field
parametrization. This is equivalent to setting the lazy training parameter α=√min the mean-field setting. Therefore, under the
NTK parametrization with width m, the bound in Corollary 2.2 indicates that the NTK approximation is valid until training time
O(m)and the error bound is O(T/√m).
3 Proof Ideas
3.1 Proof Ideas for Theorem 1.2
To provide intuition for our proof, we first outline the approach used in the original proof. Define residuals r(t),¯r(t)∈ F
under training the original rescaled model αh(w(t))and the linearized rescaled model α¯h( ¯w(t))asr(t) =y∗−αh(w(t))and
¯r(t) =y∗−α¯h( ¯w(t)). These evolve according to
dr
dt=−Ktrandd¯r
dt=−K0¯r, (13)
where Kt:=Dh(w(t))Dh(w(t))∗is the time-dependent kernel. To compare these trajectories, it was observed that, since K0is
positive semidefinite,
d
dt∥r−¯r∥2
2=−⟨r−¯r, Ktr−K0¯r⟩ ≤ −⟨ r−¯r,(Kt−K0)r⟩ (14)
which, dividing both sides by ∥r−¯r∥and using ∥r∥ ≤√R0, implies
d
dt∥r−¯r∥ ≤ ∥ Kt−K0∥∥r∥ ≤2Lip(h)Lip(Dh)∥w−w0∥p
R0. (15)
Using the Lipschitzness of the model, it was further shown that the weight change is bounded by ∥w(t)−w0∥ ≤t√R0Lip(h)/α.
Plugging this into (7) yields the bound in Proposition 1.1,
∥αh(w(T))−α¯h( ¯w(T))∥=∥r(T)−¯r(T)∥ ≤2Lip(h)2Lip(Dh)R0α−1ZT
0tdt=T2Lip(h)2Lip(Dh)R0/α. (16)
**First attempt: strengthening of the bound for long time horizons** We demonstrate how to strengthen this bound to hold for longer
time horizons by employing an improved bound on the movement of the weights. Consider the following bound on the weight
change.
**Proposition 3.1 (Bound on weight change, implicit in proof of Theorem 2.2).**
∥w(T)−w0∥ ≤p
TR0/α and∥¯w(T)−w0∥ ≤p
TR0/α. (17)
**Proof of Proposition 3.1.** By (a) Cauchy-Schwarz, and (b) the nonnegativity of the loss R,
∥w(T)−w(0)∥ ≤ZT
0dw
dtdt(a)
≤s
TZT
0dw
dt2
dt=s
−T
α2ZT
0d
dtR(αh(w(t)))dt(b)
≤p
TR0/α. (18)
The bound for ¯wis analogous.
This bound (8) has the advantage of√
tdependence (instead of linear tdependence) and does not depend on Lip(h). Plugging it
into (7), we obtain
∥αh(w(T))−α¯h( ¯w(T))∥ ≤2Lip(h)Lip(Dh)R0α−1ZT
0√
tdt=4
3T3/2Lip(h)Lip(Dh)R0/α. (19)
This improves over Proposition 1.1 for long time horizons, as the time dependence scales as T3/2instead of T2. However, it still
depends on the Lipschitz constant Lip (h)and falls short of the linear in Tdependence of Theorem 1.2.
**Second attempt: new approach to prove Theorem 1.2** To avoid dependence on Lip(h)and achieve a linear dependence in T,
we develop a new approach. We cannot use (7), which was central to the original proof, as it depends on Lip(h). Furthermore, to
achieve linear Tdependence using (7), we would need ∥w−w0∥=O(1)for a constant independent of the time horizon, which is
not true unless the problem is well-conditioned.
3
In the full proof in Appendix A, we bound ∥r(T)−¯r(T)∥, which requires working with a product integral formulation of the
dynamics of rto handle the time-varying kernels Kt. The main technical innovation in the proof is Theorem A.8, which is a new,
general bound on the difference between product integrals.
To avoid the technical complications of the appendix, we provide some intuitions here by proving a simplified theorem that does not
imply the main result. We show:
**Theorem 3.2 (Simplified variant of Theorem 1.2).** Consider r′(t)∈ F initialized as r′(0) = r(0)and evolving asdr′
dt=−KTr′.
Then,
∥r′(T)−¯r(T)∥ ≤min(3 κp
R0,8R0). (20)
Intuitively, if we can prove in Theorem 3.2 that r′(T)and¯r(T)are close, then the same should hold for r(T)and¯r(T)as in
Theorem 1.2. For convenience, define the operators
A=Dh(w0)∗and B=Dh(w(T))∗−Dh(w0)∗. (21)
Since the kernels do not vary in time, the closed-form solution is
r′(t) =e−(A+B)∗(A+B)tr(0) and ¯r(t) =e−A∗Atr(0) (22)
We prove that the time evolution operators for r′and¯rare close in operator norm.
**Lemma 3.3.** For any t≥0, we have ∥e−(A+B)∗(A+B)t−e−A∗At∥ ≤2√
t∥B∥.
**Proof of Lemma 3.3.** Define Z(ζ) = (A+ζB)∗(A+ζB)t. By the fundamental theorem of calculus,
∥e−(A+B)∗(A+B)t−e−A∗At∥=∥eZ(1)−eZ(0)∥=Z1
0d
dζeZ(ζ)dζ≤sup
ζ∈[0,1]d
dζeZ(ζ). (23)
Using the integral representation of the exponential map,
d
dζeZ(ζ)=Z1
0e(1−τ)Z(ζ)d
dζZ(ζ)
eτZ(ζ)dτ=Z1
0e(1−τ)Z(ζ)(A∗B+B∗A+ 2ζB∗B)eτZ(ζ)dτ(24)
By symmetry under transposing and reversing time, it suffices to bound the first term. Since ∥eτZ(ζ)∥ ≤1,
Z1
0e(1−τ)Z(ζ)(A+ζB)∗BeτZ(ζ)tdτ≤Z1
0∥e(1−τ)Z(ζ)(A+ζB)∗∥∥tB∥dτ≤2t/e∥B∥ ≤2√
t∥B∥ (25)
Finally, let us combine Lemma 3.3 with the weight-change bound in Proposition 3.1 to prove Theorem 3.2. Notice that the
weight-change bound in Proposition 3.1 implies
∥B∥ ≤Lip(Dh)∥w(T)−w0∥ ≤Lip(Dh)p
TR0/α. (26)
So Lemma 3.3 implies
∥r′(T)−¯r(T)∥ ≤2Lip(Dh)Tp
R0α−1∥r(0)∥= 2κ∥r(0)∥. (27)
Combining this with ∥r′(T)−¯r(T)∥ ≤ ∥ r′(T)∥+∥¯r(T)∥ ≤2√2R0implies (9). Thus, we have shown Theorem 3.2, which is the
result of Theorem 1.2 if we replace rbyr′. The actual proof of the theorem handles the time-varying kernel Ktand is in Appendix
A.
3.2 Proof Ideas for Theorem 1.3
The converse in Theorem 1.3 is achieved in the simple case where h(w) =aw+1
2bw2fora= 1/√
Tandb=Lip(Dh), and
w0= 0andR(y) =1
2(y−√2R0)2, as we show in Appendix B by direct calculation.
4 Discussion
A limitation of our result is that it applies only to gradient flow, which corresponds to SGD with infinitesimally small step size.
However, larger step sizes are beneficial for generalization in practice, so it would be interesting to understand the validity of the
NTK approximation in that setting. Another limitation is that our result applies only to the square loss and not to other popular
losses such as the cross-entropy loss. Indeed, the known bounds in the setting of general losses require either a "well-conditioning"
assumption or taking αexponential in the training time T. Can one prove bounds analogous to Theorem 1.2 for more general losses,
withαdepending polynomially on T, and without conditioning assumptions?
A natural question raised by our bounds in Theorems 1.2 and 1.3 is: how do the dynamics behave just outside the regime where the
NTK approximation is valid? For models hwhere Lip(h)andLip(Dh)are bounded by a constant, can we understand the dynamics
in the regime where T≈Cαfor some large constant Candα≫C, at the edge of the lazy training regime?
4